import asyncio
import datetime
import logging
import os
import time  # æ–°å¢å¯¼å…¥ time ç”¨äºç”Ÿæˆæ–‡ä»¶å
import tomllib
from dataclasses import dataclass
from pathlib import Path
from typing import List, Dict, Any, Optional, Union  # Added Union

import akshare as ak
import matplotlib
import matplotlib.pyplot as plt
import pandas as pd  # Ensure pandas is imported
from ncatbot.core.message import GroupMessage
from ncatbot.plugin import BasePlugin, CompatibleEnrollment
from tabulate import tabulate  # For formatting tables

matplotlib.use("Agg")  # Use Agg backend for non-GUI environments
bot = CompatibleEnrollment  # å…¼å®¹å›è°ƒå‡½æ•°æ³¨å†Œå™¨

# è·å– logger å®ä¾‹
logger = logging.getLogger(__name__)

# Configure Matplotlib for CJK font
# Provide a list of potential CJK fonts, matplotlib will use the first one found.
plt.rcParams["font.sans-serif"] = [
    "SimHei",
    "PingFang SC",
    "Heiti SC",
    "STHeiti",
    "Microsoft YaHei",
]
plt.rcParams["axes.unicode_minus"] = False  # Handle negative signs correctly
# Verify the actually used font (optional)
# from matplotlib.font_manager import findfont, FontProperties
# font_path = findfont(FontProperties(family=plt.rcParams['font.sans-serif']))
# logger.info(f"Matplotlib is using font: {font_path}")
logger.info(
    f"Attempted to set Matplotlib font to one of: {plt.rcParams['font.sans-serif']}"
)


@dataclass
class Config:
    whitelist_groups: List[int]
    whitelist_users: List[int]

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "Config":
        return cls(
            whitelist_groups=data.get("whitelist", {}).get("group_ids", []),
            whitelist_users=data.get("whitelist", {}).get("user_ids", []),
        )


async def generate_stock_chart(
        df: pd.DataFrame, stock_code: str, days: int = 90
) -> Optional[str]:
    """ä¸ºæœ€è¿‘ N å¤©çš„æ•°æ®ç”Ÿæˆæ”¶ç›˜ä»·å›¾è¡¨å¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
    if df.empty:
        logger.warning(f"æ— æ³•ä¸º {stock_code} ç”Ÿæˆå›¾è¡¨ï¼šæ•°æ®ä¸ºç©ºã€‚")
        return None

    df_chart = df.tail(days).copy()
    if df_chart.empty:
        logger.warning(f"æ— æ³•ä¸º {stock_code} ç”Ÿæˆå›¾è¡¨ï¼šæœ€è¿‘ {days} å¤©æ•°æ®ä¸ºç©ºã€‚")
        return None

    if "æ—¥æœŸ" not in df_chart.columns or "æ”¶ç›˜" not in df_chart.columns:
        logger.error(f"æ— æ³•ä¸º {stock_code} ç”Ÿæˆå›¾è¡¨ï¼šç¼ºå°‘ 'æ—¥æœŸ' æˆ– 'æ”¶ç›˜' åˆ—ã€‚")
        return None

    # å®šä¹‰æ•°æ®ç›®å½•è·¯å¾„
    plugin_dir = Path(__file__).parent
    data_dir = plugin_dir / "data"
    # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    try:
        data_dir.mkdir(parents=True, exist_ok=True)
    except OSError as e:
        logger.error(f"åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {data_dir}, é”™è¯¯: {e}")
        return None

    # ç”Ÿæˆæ–‡ä»¶å (ä¾‹å¦‚: 600519_chart_1678886400.png)
    timestamp = int(time.time())
    filename = f"{stock_code}_chart_{timestamp}.png"
    filepath = data_dir / filename

    fig = None  # åˆå§‹åŒ– fig å˜é‡
    try:
        plt.style.use("seaborn-v0_8-darkgrid")
        fig, ax = plt.subplots(figsize=(10, 5))

        ax.plot(
            df_chart["æ—¥æœŸ"],
            df_chart["æ”¶ç›˜"],
            marker=".",
            linestyle="-",
            linewidth=1.5,
            label="æ”¶ç›˜ä»·",
        )

        ax.set_title(f"{stock_code} æœ€è¿‘ {len(df_chart)} äº¤æ˜“æ—¥æ”¶ç›˜ä»·èµ°åŠ¿", fontsize=14)
        ax.set_xlabel("æ—¥æœŸ", fontsize=10)
        ax.set_ylabel("ä»·æ ¼", fontsize=10)
        fig.autofmt_xdate()
        ax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter("%Y-%m-%d"))
        ax.legend()
        ax.grid(True, linestyle="--", alpha=0.6)

        # ä¿å­˜åˆ°æ–‡ä»¶
        plt.savefig(filepath, format="png", bbox_inches="tight", dpi=100)
        plt.close(fig)  # å…³é—­å›¾å½¢é‡Šæ”¾å†…å­˜

        logger.info(f"æˆåŠŸä¸º {stock_code} ç”Ÿæˆå›¾è¡¨å¹¶ä¿å­˜è‡³: {filepath}")
        return str(filepath)  # è¿”å›æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²
    except Exception as e:
        logger.error(f"ä¸º {stock_code} ç”Ÿæˆæˆ–ä¿å­˜å›¾è¡¨æ—¶å‡ºé”™: {e}", exc_info=True)
        if fig:  # å¦‚æœ fig å·²åˆ›å»ºä½†å‡ºé”™ï¼Œå°è¯•å…³é—­
            plt.close(fig)
        # å¦‚æœæ–‡ä»¶å·²éƒ¨åˆ†åˆ›å»ºï¼Œå°è¯•åˆ é™¤é¿å…æ®‹ç•™
        if filepath.exists():
            try:
                os.remove(filepath)
            except OSError:
                logger.error(f"å°è¯•åˆ é™¤å¤±è´¥çš„å›¾è¡¨æ–‡ä»¶å¤±è´¥: {filepath}")
        return None


async def fetch_stock_historical_data(
        stock_code: str,
        period: str = "daily",
        start_date: str = "19700101",
        end_date: str = "20500101",
        adjust: str = "",
) -> Union[pd.DataFrame, str]:
    """
    è·å–è‚¡ç¥¨å†å²æ•°æ® DataFrame æˆ–é”™è¯¯ä¿¡æ¯å­—ç¬¦ä¸²ã€‚
    """
    valid_periods = ["daily", "weekly", "monthly"]
    valid_adjusts = ["qfq", "hfq", ""]

    if period not in valid_periods:
        return f"âŒ é”™è¯¯ï¼šæ— æ•ˆçš„å‘¨æœŸ '{period}'ã€‚æ”¯æŒ: {', '.join(valid_periods)}"
    if adjust not in valid_adjusts:
        return f"âŒ é”™è¯¯ï¼šæ— æ•ˆçš„å¤æƒç±»å‹ '{adjust}'ã€‚æ”¯æŒ: qfq (å‰å¤æƒ), hfq (åå¤æƒ), '' (ä¸å¤æƒ)"

    try:
        logger.info(
            f"æ­£åœ¨æŸ¥è¯¢å†å²æ•°æ®: code={stock_code}, period={period}, start={start_date}, end={end_date}, adjust={adjust}"
        )
        df = ak.stock_zh_a_hist(
            symbol=stock_code,
            period=period,
            start_date=start_date,
            end_date=end_date,
            adjust=adjust,
        )

        if df.empty:
            return f"âš ï¸ æœªèƒ½è·å–è‚¡ç¥¨ä»£ç  {stock_code} åœ¨æŒ‡å®šæ¡ä»¶ä¸‹çš„å†å²æ•°æ®ã€‚"

        # Ensure 'æ—¥æœŸ' is datetime for potential sorting/filtering later
        df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"])
        df.sort_values(by="æ—¥æœŸ", inplace=True)

        return df
    except Exception as e:
        logger.error(f"æŸ¥è¯¢è‚¡ç¥¨ {stock_code} å†å²æ•°æ®æ—¶å‡ºé”™: {e}")
        return f"âŒ æŸ¥è¯¢è‚¡ç¥¨ {stock_code} å†å²æ•°æ®æ—¶å‡ºé”™: {e}"


def format_historical_data_text(
        df: pd.DataFrame,
        stock_code: str,
        period: str,
        adjust: str,
        max_rows: int = 30,
) -> str:
    """å°† DataFrame æ ¼å¼åŒ–ä¸ºå¯¹é½çš„æ–‡æœ¬è¡¨æ ¼"""
    if df.empty:
        return f"âš ï¸ æ²¡æœ‰ä¸º {stock_code} æ‰¾åˆ°å†å²æ•°æ®æ¥æ ¼å¼åŒ–ã€‚"

    # å‡†å¤‡æ˜¾ç¤ºçš„æ•°æ®
    df_display = df.tail(max_rows).copy()  # ä½¿ç”¨ .copy() é¿å… SettingWithCopyWarning

    # ç§»é™¤é‡å¤çš„è‚¡ç¥¨ä»£ç åˆ—
    if "è‚¡ç¥¨ä»£ç " in df_display.columns:
        df_display.drop(columns=["è‚¡ç¥¨ä»£ç "], inplace=True)

    # è½¬æ¢æ—¥æœŸæ ¼å¼ä»¥ä¾¿æ˜¾ç¤º
    df_display["æ—¥æœŸ"] = df_display["æ—¥æœŸ"].dt.strftime("%Y-%m-%d")

    # è®¾ç½®æ ‡é¢˜ - æ·»åŠ å›¾æ ‡ ğŸ“ˆ
    actual_rows = len(df_display)
    title_period = f"{period}, {adjust or 'ä¸å¤æƒ'}"
    if actual_rows < len(df):
        title = f"ğŸ“ˆ è‚¡ç¥¨ {stock_code} æœ€è¿‘ {actual_rows} æ¡å†å²æ•°æ® ({title_period}):"
    else:
        title = f"ğŸ“ˆ è‚¡ç¥¨ {stock_code} å†å²æ•°æ® ({actual_rows} æ¡, {title_period}):"

    # ä½¿ç”¨ tabulate æ ¼å¼åŒ–è¡¨æ ¼
    # æ³¨æ„ï¼šåœ¨è¡¨å¤´ä¸­æ·»åŠ å›¾æ ‡å¯èƒ½ä¼šå½±å“å¯¹é½ï¼Œæ‰€ä»¥è¿™é‡Œä¿æŒè¡¨å¤´å¹²å‡€
    headers = {  # å®šä¹‰æ›´æ˜“è¯»çš„ä¸­æ–‡è¡¨å¤´
        "æ—¥æœŸ": "ğŸ“… æ—¥æœŸ",
        "å¼€ç›˜": "å¼€ç›˜ä»·",
        "æ”¶ç›˜": "æ”¶ç›˜ä»·",
        "æœ€é«˜": "æœ€é«˜ä»·",
        "æœ€ä½": "æœ€ä½ä»·",
        "æˆäº¤é‡": "æˆäº¤é‡(æ‰‹)",
        "æˆäº¤é¢": "æˆäº¤é¢(å…ƒ)",
        "æŒ¯å¹…": "æŒ¯å¹…(%)",
        "æ¶¨è·Œå¹…": "æ¶¨è·Œå¹…(%)",
        "æ¶¨è·Œé¢": "æ¶¨è·Œé¢",
        "æ¢æ‰‹ç‡": "æ¢æ‰‹ç‡(%)",
    }
    # é‡å‘½ååˆ—ä»¥åŒ¹é…æ–°çš„è¡¨å¤´
    df_display.rename(columns=headers, inplace=True)

    table_str = tabulate(
        df_display,
        headers="keys",  # ä½¿ç”¨åˆ—åä½œä¸ºè¡¨å¤´
        tablefmt="simple",  # æ”¹ç”¨ 'simple' æ ¼å¼
        showindex=False,  # ä¸æ˜¾ç¤º DataFrame ç´¢å¼•
        numalign="left",  # å¼ºåˆ¶æ•°å­—å·¦å¯¹é½
        stralign="left",  # å­—ç¬¦ä¸²å·¦å¯¹é½ (é»˜è®¤)
        floatfmt=".2f",  # æµ®ç‚¹æ•°æ ¼å¼
    )

    response = f"{title}\n" f"-------------------------------------\n" f"{table_str}"

    # å†æ¬¡æ£€æŸ¥æ€»é•¿åº¦ï¼Œå¦‚æœå¤ªé•¿å¯èƒ½éœ€è¦è¿›ä¸€æ­¥æˆªæ–­æˆ–æç¤º
    if len(response) > 2000:
        logger.warning(
            f"æ ¼å¼åŒ–åçš„å†å²æ•°æ®å“åº”è¿‡é•¿ ({len(response)} chars)ï¼Œå¯èƒ½æ— æ³•å®Œæ•´å‘é€ã€‚"
        )
        response += "\n(âš ï¸ æ•°æ®è¿‡å¤šï¼Œå¯èƒ½æ˜¾ç¤ºä¸å…¨)"  # æ·»åŠ æç¤º

    return response


async def generate_historical_data_table_image(
        df: pd.DataFrame, stock_code: str, max_rows: int = 30
) -> Optional[str]:
    """å°† DataFrame æ¸²æŸ“ä¸ºå†å²æ•°æ®è¡¨æ ¼å›¾ç‰‡å¹¶ä¿å­˜ã€‚"""
    if df.empty:
        logger.warning(f"æ— æ³•ä¸º {stock_code} ç”Ÿæˆè¡¨æ ¼å›¾ç‰‡ï¼šæ•°æ®ä¸ºç©ºã€‚")
        return None

    df_display = df.tail(max_rows).copy()

    # --- Sort by date descending (most recent first) ---
    if "æ—¥æœŸ" in df_display.columns:
        # Ensure 'æ—¥æœŸ' is datetime for sorting, then sort
        try:
            df_display["æ—¥æœŸ"] = pd.to_datetime(df_display["æ—¥æœŸ"])
            df_display.sort_values(by="æ—¥æœŸ", ascending=False, inplace=True)
        except Exception as sort_e:
            logger.error(f"Sorting by date failed for {stock_code}: {sort_e}")
            # Proceed without sorting if error occurs
    else:
        logger.warning(f"Cannot sort by date for {stock_code}: 'æ—¥æœŸ' column missing.")
    # --- End Sorting ---

    # é€‰æ‹©å¹¶é‡å‘½ååˆ—ä»¥åŒ¹é…æˆªå›¾æ ¼å¼
    headers_map = {
        "æ—¥æœŸ": "æ—¥æœŸ",  # ç§»é™¤ emoji ç®€åŒ–
        "å¼€ç›˜": "å¼€ç›˜ä»·",
        "æ”¶ç›˜": "æ”¶ç›˜ä»·",
        "æœ€é«˜": "æœ€é«˜ä»·",
        "æœ€ä½": "æœ€ä½ä»·",
        "æˆäº¤é‡": "æˆäº¤é‡(æ‰‹)",
        "æˆäº¤é¢": "æˆäº¤é¢(äº¿å…ƒ)",  # æ”¹ä¸ºäº¿å…ƒ
        "æŒ¯å¹…": "æŒ¯å¹…(%)",
        "æ¶¨è·Œå¹…": "æ¶¨è·Œå¹…(%)",
        "æ¶¨è·Œé¢": "æ¶¨è·Œé¢",
        "æ¢æ‰‹ç‡": "æ¢æ‰‹ç‡(%)",
    }
    required_cols = list(headers_map.keys())
    missing_cols = [col for col in required_cols if col not in df_display.columns]
    if missing_cols:
        logger.error(f"æ— æ³•ä¸º {stock_code} ç”Ÿæˆè¡¨æ ¼å›¾ç‰‡ï¼šç¼ºå°‘åˆ— {missing_cols}ã€‚")
        return None

    # --- æ•°æ®æ ¼å¼åŒ– --- Start
    # è½¬æ¢æˆäº¤é¢å•ä½ï¼ˆåœ¨é‡å‘½åæ˜ å°„ä¹‹å‰æ“ä½œåŸå§‹åˆ—åï¼‰
    if "æˆäº¤é¢" in df_display.columns:
        df_display["æˆäº¤é¢"] = df_display["æˆäº¤é¢"] / 100_000_000  # Convert to äº¿å…ƒ

    df_display = df_display[required_cols].copy()  # æŒ‰é¡ºåºé€‰æ‹©åˆ—ï¼Œä½¿ç”¨ .copy()
    df_display.rename(columns=headers_map, inplace=True)

    df_display["æ—¥æœŸ"] = pd.to_datetime(df_display["æ—¥æœŸ"]).dt.strftime("%Y-%m-%d")

    # æ ¼å¼åŒ–æµ®ç‚¹æ•°åˆ—
    float_cols_format = {
        "å¼€ç›˜ä»·": "{:.2f}",
        "æ”¶ç›˜ä»·": "{:.2f}",
        "æœ€é«˜ä»·": "{:.2f}",
        "æœ€ä½ä»·": "{:.2f}",
        "æˆäº¤é¢(äº¿å…ƒ)": "{:.2f}",  # äº¿å…ƒä¿ç•™ä¸¤ä½å°æ•°
        "æŒ¯å¹…(%)": "{:.2f}",
        "æ¢æ‰‹ç‡(%)": "{:.2f}",
    }
    # ç‰¹æ®Šå¤„ç†å¸¦ç¬¦å·çš„åˆ—
    signed_cols = ["æ¶¨è·Œå¹…(%)", "æ¶¨è·Œé¢"]

    for col, fmt in float_cols_format.items():
        if col in df_display.columns:
            df_display[col] = df_display[col].map(
                lambda x: fmt.format(x) if pd.notna(x) else ""
            )

    for col in signed_cols:
        if col in df_display.columns:
            df_display[col] = df_display[col].apply(
                lambda x: (
                    f"+{x:.2f}"
                    if pd.notna(x) and x > 0
                    else (
                        f"{x:.2f}"
                        if pd.notna(x) and x < 0
                        else ("0.00" if pd.notna(x) else "")
                    )
                )
            )

    if "æˆäº¤é‡(æ‰‹)" in df_display.columns:
        df_display["æˆäº¤é‡(æ‰‹)"] = df_display["æˆäº¤é‡(æ‰‹)"].map(
            lambda x: "{:,.0f}".format(x) if pd.notna(x) else ""
        )  # å¸¦é€—å·çš„æ•´æ•°
    # --- æ•°æ®æ ¼å¼åŒ– --- End

    plugin_dir = Path(__file__).parent
    data_dir = plugin_dir / "data"
    try:
        data_dir.mkdir(parents=True, exist_ok=True)
    except OSError as e:
        logger.error(f"åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {data_dir}, é”™è¯¯: {e}")
        return None

    timestamp = int(time.time())
    filename = f"{stock_code}_hist_table_{timestamp}.png"
    filepath = data_dir / filename

    # --- Matplotlib ç»˜å›¾ --- Start
    # å¢åŠ  figsize å®½åº¦ï¼Œå¹¶æ ¹æ®è¡Œæ•°è°ƒæ•´é«˜åº¦
    fig, ax = plt.subplots(figsize=(16, max(5, len(df_display) * 0.45)))
    ax.axis("tight")
    ax.axis("off")

    table_data = df_display.values.tolist()
    col_labels = df_display.columns.tolist()

    # åˆ›å»ºè¡¨æ ¼ï¼Œè°ƒæ•´ cellLoc å’Œ colWidths
    the_table = plt.table(
        cellText=table_data,
        colLabels=col_labels,
        loc="center",
        cellLoc="center",  # å°è¯•å±…ä¸­å¯¹é½æ‰€æœ‰å•å…ƒæ ¼
        colLoc="center",
    )

    the_table.auto_set_font_size(False)
    the_table.set_fontsize(10)
    the_table.scale(1.1, 1.6)  # å¾®è°ƒç¼©æ”¾æ¯”ä¾‹

    # å°è¯•è‡ªåŠ¨è®¾ç½®åˆ—å®½ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨è°ƒæ•´æˆ–æ›´å¤æ‚çš„é€»è¾‘
    # the_table.auto_set_column_width(col=list(range(len(col_labels))))

    # --- æ·»åŠ æ¶¨è·Œé¢œè‰² --- Start
    cells = the_table.get_celld()
    # è·å–åˆ—ç´¢å¼•ï¼Œç¡®ä¿åˆ—å­˜åœ¨
    col_indices = {name: i for i, name in enumerate(col_labels)}
    æ¶¨è·Œå¹…_idx = col_indices.get("æ¶¨è·Œå¹…(%)", -1)
    æ¶¨è·Œé¢_idx = col_indices.get("æ¶¨è·Œé¢", -1)

    for i in range(len(df_display)):
        row_idx = i + 1  # Table cell row index starts from 1 (0 is header)
        if æ¶¨è·Œå¹…_idx != -1:
            cell = cells[(row_idx, æ¶¨è·Œå¹…_idx)]
            text = cell.get_text().get_text()
            if text.startswith("+"):
                cell.get_text().set_color("red")
            elif text.startswith("-"):
                cell.get_text().set_color("green")

        if æ¶¨è·Œé¢_idx != -1:
            cell = cells[(row_idx, æ¶¨è·Œé¢_idx)]
            text = cell.get_text().get_text()
            if text.startswith("+"):
                cell.get_text().set_color("red")
            elif text.startswith("-"):
                cell.get_text().set_color("green")
    # --- æ·»åŠ æ¶¨è·Œé¢œè‰² --- End

    title_start_date = df_display["æ—¥æœŸ"].iloc[0]
    title_end_date = df_display["æ—¥æœŸ"].iloc[-1]
    plt.title(
        f"{stock_code} å†å²æ•°æ® ({title_start_date} è‡³ {title_end_date})",
        fontsize=16,
        pad=20,
    )

    try:
        plt.savefig(filepath, bbox_inches="tight", dpi=120)
        plt.close(fig)
        logger.info(f"æˆåŠŸä¸º {stock_code} ç”Ÿæˆè¡¨æ ¼å›¾ç‰‡å¹¶ä¿å­˜è‡³: {filepath}")
        return str(filepath)
    except Exception as e:
        logger.error(f"ä¸º {stock_code} ä¿å­˜è¡¨æ ¼å›¾ç‰‡æ—¶å‡ºé”™: {e}", exc_info=True)
        if fig:
            plt.close(fig)
        if filepath.exists():
            try:
                os.remove(filepath)
            except OSError:
                logger.error(f"å°è¯•åˆ é™¤å¤±è´¥çš„è¡¨æ ¼æ–‡ä»¶å¤±è´¥: {filepath}")
        return None
    # --- Matplotlib ç»˜å›¾ --- End


async def handle_historical_command(cmd: str) -> List[Dict[str, str]]:
    """
    å¤„ç†å†å²æ•°æ®å‘½ä»¤ï¼Œè·å–æ•°æ®ã€ç”Ÿæˆè¡¨æ ¼å›¾ç‰‡ï¼Œå¹¶è¿”å›å¾…å‘é€æ¶ˆæ¯åˆ—è¡¨ã€‚
    è¿”å›æ ¼å¼: [{'image': path}] æˆ– [{'text': error_msg}]
    """
    hist_parts = cmd.strip().split()
    if not hist_parts:
        return [
            {
                "text": "âŒ å†å²å‘½ä»¤é”™è¯¯ï¼šéœ€è¦æä¾›è‚¡ç¥¨ä»£ç ã€‚\næ ¼å¼ï¼šå†å² <ä»£ç > [å‘¨æœŸ] [å¼€å§‹] [ç»“æŸ] [å¤æƒ]"
            }
        ]

    stock_code = hist_parts[0]
    period = hist_parts[1] if len(hist_parts) > 1 else "daily"
    start_date = hist_parts[2] if len(hist_parts) > 2 else "19700101"
    end_date = hist_parts[3] if len(hist_parts) > 3 else "20500101"
    adjust = hist_parts[4] if len(hist_parts) > 4 else ""

    messages_to_send = []

    # 1. è·å–æ•°æ®
    data_result = await fetch_stock_historical_data(
        stock_code, period, start_date, end_date, adjust
    )

    # 2. å¤„ç†ç»“æœ
    if isinstance(data_result, str):  # å¦‚æœè¿”å›çš„æ˜¯é”™è¯¯å­—ç¬¦ä¸²
        messages_to_send.append({"text": data_result})
    elif isinstance(data_result, pd.DataFrame):
        # 2.1 ç”Ÿæˆè¡¨æ ¼å›¾ç‰‡
        try:
            table_image_path = await generate_historical_data_table_image(
                data_result, stock_code, max_rows=75  # æœ€å¤šæ˜¾ç¤º75è¡Œ
            )
            if table_image_path:
                messages_to_send.append({"image": table_image_path})
            else:
                # å¦‚æœå›¾ç‰‡ç”Ÿæˆå¤±è´¥ï¼Œå°è¯•å‘é€åŸå§‹æ–‡æœ¬ï¼ˆä½œä¸ºåå¤‡ï¼‰
                logger.warning(
                    f"æœªèƒ½ä¸º {stock_code} ç”Ÿæˆå†å²æ•°æ®è¡¨æ ¼å›¾ç‰‡ã€‚å°è¯•å‘é€æ–‡æœ¬ã€‚"
                )
                try:
                    text_table = format_historical_data_text(  # éœ€è¦ä¿ç•™æˆ–é‡æ–°å®ç° format_historical_data_text
                        data_result, stock_code, period, adjust, max_rows=30
                    )
                    messages_to_send.append(
                        {"text": text_table + "\n(âš ï¸ å›¾ç‰‡ç”Ÿæˆå¤±è´¥)"}
                    )
                except Exception as fmt_e:
                    logger.error(f"ç”Ÿæˆè¡¨æ ¼å›¾ç‰‡å’Œæ–‡æœ¬å‡å¤±è´¥: {fmt_e}")
                    messages_to_send.append(
                        {"text": "âŒ ç”Ÿæˆå†å²æ•°æ®è¡¨æ ¼å›¾ç‰‡å’Œæ–‡æœ¬æ—¶å‡å‡ºé”™ã€‚"}
                    )

        except Exception as e:
            logger.error(f"ç”Ÿæˆå†å²æ•°æ®è¡¨æ ¼å›¾ç‰‡æ—¶å‡ºé”™: {e}", exc_info=True)
            messages_to_send.append({"text": "âŒ ç”Ÿæˆå†å²æ•°æ®è¡¨æ ¼å›¾ç‰‡æ—¶å‡ºé”™ã€‚"})

    return messages_to_send


async def get_stock_realtime_data(cmd: str) -> List[Dict[str, str]]:
    """æŸ¥è¯¢å®æ—¶æ•°æ®å¹¶è¿”å›å¾…å‘é€æ¶ˆæ¯åˆ—è¡¨"""
    stock_code = cmd.strip().split()[0] if cmd.strip() else None
    if not stock_code:
        return [{"text": "âŒ é”™è¯¯ï¼šéœ€è¦æä¾›è‚¡ç¥¨ä»£ç ã€‚"}]
    try:
        # æ³¨æ„ï¼šä¹‹å‰çš„ä»£ç ç”¨äº† stock_bid_ask_emï¼Œè¿™é‡Œæ”¹å› stock_zh_a_spot_em
        # å› ä¸º stock_bid_ask_em è¿”å›çš„æ˜¯ä¹°å–ç›˜ï¼Œå­—æ®µä¸åŒ
        df_realtime = ak.stock_zh_a_spot_em()
        stock_data = df_realtime[df_realtime["ä»£ç "] == stock_code]
        if stock_data.empty:
            return [{"text": f"âš ï¸ æœªèƒ½æ‰¾åˆ°è‚¡ç¥¨ä»£ç  {stock_code} çš„å®æ—¶æ•°æ®ã€‚"}]

        data = stock_data.iloc[0]
        response = (
            f"**â±ï¸ {data['åç§°']} ({stock_code}) å®æ—¶æ•°æ®**\n"
            f"---------------------------\n"
            f"ğŸ’° æœ€æ–°: {data['æœ€æ–°ä»·']:.2f} | æ¶¨è·Œ: {data['æ¶¨è·Œé¢']:.2f} ({data['æ¶¨è·Œå¹…']:.2f}%)\n"
            f"ğŸ“ˆ ä»Šå¼€: {data['ä»Šå¼€']:.2f} | æœ€é«˜: {data['æœ€é«˜']:.2f}\n"
            f"ğŸ“‰ æœ€ä½: {data['æœ€ä½']:.2f} | æ˜¨æ”¶: {data['æ˜¨æ”¶']:.2f}\n"
            f"ğŸ“Š æˆäº¤é‡: {data['æˆäº¤é‡'] / 10000:.2f} ä¸‡æ‰‹\n"
            f"ğŸ“Š æˆäº¤é¢: {data['æˆäº¤é¢'] / 100000000:.2f} äº¿å…ƒ\n"
            f"ğŸ”„ æ¢æ‰‹ç‡: {data['æ¢æ‰‹ç‡']:.2f}%\n"
            f"ğŸ’¹ å¸‚ç›ˆ(åŠ¨): {data['å¸‚ç›ˆç‡-åŠ¨æ€']:.2f} | å¸‚å‡€ç‡: {data['å¸‚å‡€ç‡']:.2f}\n"
            f"ğŸ¦ æ€»å¸‚å€¼: {data['æ€»å¸‚å€¼'] / 100000000:.2f} äº¿\n"
            f"ğŸ¦ æµé€šå€¼: {data['æµé€šå¸‚å€¼'] / 100000000:.2f} äº¿"
        )
        return [{"text": response}]
    except Exception as e:
        logger.error(f"æŸ¥è¯¢è‚¡ç¥¨ {stock_code} å®æ—¶æ•°æ®æ—¶å‡ºé”™: {e}")
        return [{"text": f"âŒ æŸ¥è¯¢è‚¡ç¥¨ {stock_code} å®æ—¶æ•°æ®æ—¶å‡ºé”™: {e}"}]


async def get_stock_news(cmd: str) -> List[Dict[str, str]]:
    """æŸ¥è¯¢ä¸ªè‚¡æ–°é—»å¹¶è¿”å›æ ¼å¼åŒ–çš„æ¶ˆæ¯åˆ—è¡¨"""
    stock_code = cmd.strip().split()[0] if cmd.strip() else None
    if not stock_code:
        return [{"text": "âŒ é”™è¯¯ï¼šéœ€è¦æä¾›è‚¡ç¥¨ä»£ç ã€‚"}]

    try:
        logger.info(f"æ­£åœ¨æŸ¥è¯¢è‚¡ç¥¨ä»£ç  {stock_code} çš„æ–°é—»...")
        # è°ƒç”¨ akshare è·å–æ–°é—»æ•°æ®
        news_df = ak.stock_news_em(symbol=stock_code)

        if news_df.empty:
            return [{"text": f"âš ï¸ æœªæ‰¾åˆ°è‚¡ç¥¨ä»£ç  {stock_code} çš„ç›¸å…³æ–°é—»ã€‚"}]

        # ç¡®ä¿ 'å‘å¸ƒæ—¶é—´' åˆ—å­˜åœ¨ä¸”è½¬æ¢ä¸º datetime å¯¹è±¡ç”¨äºæ’åº
        if "å‘å¸ƒæ—¶é—´" in news_df.columns:
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶åˆ—å­˜å‚¨ datetime å¯¹è±¡ï¼Œå¤„ç†å¯èƒ½çš„è½¬æ¢é”™è¯¯
            news_df["å‘å¸ƒæ—¶é—´_dt"] = pd.to_datetime(
                news_df["å‘å¸ƒæ—¶é—´"], errors="coerce"
            )
            # æŒ‰ datetime å¯¹è±¡é™åºæ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
            news_df.sort_values(
                by="å‘å¸ƒæ—¶é—´_dt", ascending=False, inplace=True, na_position="last"
            )
            # å¯ä»¥é€‰æ‹©åˆ é™¤ä¸´æ—¶åˆ—ï¼Œå¦‚æœåç»­æ ¼å¼åŒ–ä¸ç›´æ¥ä½¿ç”¨å®ƒ
            # news_df.drop(columns=['å‘å¸ƒæ—¶é—´_dt'], inplace=True)
        else:
            logger.warning(
                f"è‚¡ç¥¨ {stock_code} çš„æ–°é—»æ•°æ®ä¸­ç¼ºå°‘ 'å‘å¸ƒæ—¶é—´' åˆ—ï¼Œæ— æ³•æ’åºã€‚"
            )

        # åªé€‰å–æœ€æ–°çš„ N æ¡æ–°é—»è¿›è¡Œå±•ç¤º
        max_news = 20  # ä½¿ç”¨ä¹‹å‰è®¾ç½®çš„ 20 æ¡
        news_to_display = news_df.head(max_news)  # åœ¨æ’åºåè¿›è¡Œæˆªæ–­

        response_lines = [f"ğŸ“° {stock_code} ç›¸å…³æ–°é—» (æœ€è¿‘ {len(news_to_display)} æ¡):"]
        response_lines.append("---------------------------\n")

        for index, row in news_to_display.iterrows():
            # æ ¼å¼åŒ–å•æ¡æ–°é—»ï¼šæ ‡é¢˜ (æ¥æº @ æ—¶é—´) \n é“¾æ¥
            # æˆªæ–­é•¿æ ‡é¢˜
            title = row["æ–°é—»æ ‡é¢˜"]
            if len(title) > 40:
                title = title[:38] + "..."  # é™åˆ¶æ ‡é¢˜é•¿åº¦

            # æ ¼å¼åŒ–æ—¶é—´æˆ³ (akshare è¿”å›çš„æ˜¯ YYYY-MM-DD HH:MM:SS å­—ç¬¦ä¸²)
            publish_time_str = row["å‘å¸ƒæ—¶é—´"]
            # å°è¯•è§£æå¹¶æ ¼å¼åŒ–ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²
            try:
                # å°è¯•ä» datetime åˆ—è·å–æ ¼å¼åŒ–æ—¶é—´ï¼Œå¦‚æœè¯¥åˆ—ä¸å­˜åœ¨æˆ–å€¼ä¸º NaTï¼Œåˆ™å›é€€åˆ°å­—ç¬¦ä¸²åˆ—
                if "å‘å¸ƒæ—¶é—´_dt" in row and pd.notna(row["å‘å¸ƒæ—¶é—´_dt"]):
                    publish_time = row["å‘å¸ƒæ—¶é—´_dt"].strftime("%Y-%m-%d %H:%M")
                else:
                    # å¦‚æœ dt åˆ—æ— æ•ˆï¼Œå†å°è¯•è½¬æ¢åŸå§‹å­—ç¬¦ä¸²
                    publish_time = pd.to_datetime(publish_time_str).strftime(
                        "%Y-%m-%d %H:%M"
                    )
            except (ValueError, TypeError):
                publish_time = publish_time_str  # ä¿ç•™åŸå§‹æ ¼å¼

            news_line = (
                f"â–ªï¸ {title} \n"
                f"  <æ¥æº: {row['æ–‡ç« æ¥æº']} @ {publish_time}>\n"
                f"  <é“¾æ¥: {row['æ–°é—»é“¾æ¥']}>"
            )
            response_lines.append(news_line)
            response_lines.append("---")  # æ·»åŠ åˆ†éš”ç¬¦

        # ç§»é™¤æœ€åä¸€ä¸ªåˆ†éš”ç¬¦
        if response_lines and response_lines[-1] == "---":
            response_lines.pop()

        final_response = "\n".join(response_lines)

        # æ£€æŸ¥æœ€ç»ˆæ¶ˆæ¯é•¿åº¦
        if len(final_response) > 2000:
            logger.warning(
                f"æ–°é—»æ¶ˆæ¯è¿‡é•¿ ({len(final_response)} chars)ï¼Œå¯èƒ½æ— æ³•å®Œæ•´å‘é€ã€‚"
            )
            # å¯ä»¥è€ƒè™‘è¿›ä¸€æ­¥å‡å°‘æ–°é—»æ¡æ•°æˆ–æˆªæ–­å†…å®¹
            # ä¼°ç®—æˆªæ–­ä½ç½®ï¼Œä¾‹å¦‚ä¿ç•™æ ‡é¢˜å’Œ N æ¡æ–°é—»
            estimated_lines_per_news = 3  # æ ‡é¢˜è¡Œ+æ¥æº/æ—¶é—´è¡Œ+é“¾æ¥è¡Œ+åˆ†éš”ç¬¦ (å¤§çº¦)
            # lines_to_keep = 2 + max_news * estimated_lines_per_news
            # ä¸ºäº†å®‰å…¨ï¼Œç¨å¾®å‡å°‘ä¸€ç‚¹
            lines_to_keep = min(
                len(response_lines),
                2 + (max_news - 2) * estimated_lines_per_news if max_news > 2 else 2,
            )
            final_response = (
                    "\n".join(response_lines[:lines_to_keep]) + "\n(âš ï¸ æ–°é—»è¿‡å¤šï¼Œå·²æˆªæ–­)"
            )

        return [{"text": final_response}]

    except Exception as e:
        logger.error(f"æŸ¥è¯¢è‚¡ç¥¨ {stock_code} æ–°é—»æ—¶å‡ºé”™: {e}", exc_info=True)
        return [{"text": f"âŒ æŸ¥è¯¢è‚¡ç¥¨ {stock_code} æ–°é—»æ—¶å‡ºé”™: {e}"}]


async def get_stock_deepseek_prediction(cmd: str) -> List[Dict[str, str]]:
    stock_code = cmd.strip().split()[0] if cmd.strip() else None
    if not stock_code:
        return [{"text": "âŒ é”™è¯¯ï¼šéœ€è¦æä¾›è‚¡ç¥¨ä»£ç ã€‚"}]
    return [{"text": f"ğŸ’¡ è·å–è‚¡ç¥¨ä»£ç  {stock_code} çš„ DeepSeek é¢„æµ‹ (å¾…å®ç°)"}]


async def get_market_overview(cmd: str) -> List[Dict[str, str]]:
    """è·å–è‚¡å¸‚æ€»è²Œä¿¡æ¯å¹¶æ ¼å¼åŒ–è¿”å›"""
    results = []
    errors = []
    today_str = datetime.date.today().strftime("%Y%m%d")

    # --- è·å–ä¸Šäº¤æ‰€æ•°æ® ---
    sse_summary_df = None
    sse_daily_df = None
    try:
        sse_summary_df = await asyncio.to_thread(ak.stock_sse_summary)
        sse_daily_df = await asyncio.to_thread(ak.stock_sse_deal_daily, date=today_str)
    except Exception as e:
        logger.error(f"è·å–ä¸Šäº¤æ‰€æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
        errors.append("è·å–ä¸Šäº¤æ‰€æ•°æ®å¤±è´¥")

    # --- è·å–æ·±äº¤æ‰€æ•°æ® ---
    szse_summary_df = None
    try:
        szse_summary_df = await asyncio.to_thread(ak.stock_szse_summary, date=today_str)
    except Exception as e:
        logger.error(f"è·å–æ·±äº¤æ‰€æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
        errors.append("è·å–æ·±äº¤æ‰€æ•°æ®å¤±è´¥")

    # --- è¾…åŠ©æ ¼å¼åŒ–å‡½æ•° ---
    def format_to_trillion(raw_value, original_unit: str = "yuan"):
        """å°†å…ƒæˆ–äº¿å…ƒæ ¼å¼åŒ–ä¸ºä¸‡äº¿"""
        if raw_value is None or pd.isna(raw_value):
            return "N/A"
        try:
            value = float(raw_value)
            if original_unit == "billion_yuan":
                # åŸå§‹å•ä½æ˜¯äº¿ï¼Œé™¤ä»¥ 10000 è½¬ä¸ºä¸‡äº¿
                trillion_value = value / 10000
            elif original_unit == "yuan":
                # åŸå§‹å•ä½æ˜¯å…ƒï¼Œé™¤ä»¥ 1e12 è½¬ä¸ºä¸‡äº¿
                trillion_value = value / 1e12
            else:
                logger.warning(f"Unsupported original unit: {original_unit}")
                return "N/A"
            return f"{trillion_value:.2f} ä¸‡äº¿"
        except (ValueError, TypeError):
            logger.warning(
                f"Could not convert '{raw_value}' to float for trillion formatting."
            )
            return "N/A"

    # --- æ ¼å¼åŒ–ä¸Šäº¤æ‰€æ•°æ® --- (åŸå§‹å•ä½ï¼šäº¿å…ƒ)
    if sse_summary_df is not None and sse_daily_df is not None:
        sse_summary_dict = sse_summary_df.set_index("é¡¹ç›®").to_dict()
        sse_daily_dict = sse_daily_df.set_index("å•æ—¥æƒ…å†µ").to_dict()

        # ä½¿ç”¨å›¾æ ‡ ğŸ¦
        sse_results = [f"ğŸ¦ ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€æ¦‚è§ˆ ({today_str})"]
        sse_results.append("----------------------------")
        try:
            # ä½¿ç”¨ format_to_trillion å¹¶æŒ‡å®šåŸå§‹å•ä½ä¸º billion_yuan
            sse_total_cap_str = format_to_trillion(
                sse_summary_dict["è‚¡ç¥¨"]["æ€»å¸‚å€¼"], original_unit="billion_yuan"
            )
            sse_flow_cap_str = format_to_trillion(
                sse_summary_dict["è‚¡ç¥¨"]["æµé€šå¸‚å€¼"], original_unit="billion_yuan"
            )
            sse_turnover_str = format_to_trillion(
                sse_daily_dict["è‚¡ç¥¨"]["æˆäº¤é‡‘é¢"], original_unit="billion_yuan"
            )

            sse_results.append(
                f"  ğŸ¢ ä¸Šå¸‚å…¬å¸: {sse_summary_dict['è‚¡ç¥¨']['ä¸Šå¸‚å…¬å¸']} å®¶"
            )
            sse_results.append(f"  ğŸ’° æ€»å¸‚å€¼: {sse_total_cap_str}")
            sse_results.append(f"  ğŸ¦ æµé€šå¸‚å€¼: {sse_flow_cap_str}")
            sse_results.append(f"  ğŸ’¸ æˆäº¤é‡‘é¢: {sse_turnover_str}")

            # å•ç‹¬å¤„ç†æˆäº¤é‡ (è‚¡æ•°)
            sse_volume_raw = sse_daily_dict["è‚¡ç¥¨"]["æˆäº¤é‡"]
            try:
                sse_volume = float(sse_volume_raw)
                if abs(sse_volume) >= 1e8:
                    sse_volume_str = f"{sse_volume / 1e8:.2f} äº¿è‚¡"
                elif abs(sse_volume) >= 1e4:
                    sse_volume_str = f"{sse_volume / 1e4:.2f} ä¸‡è‚¡"
                else:
                    sse_volume_str = f"{sse_volume:,.0f} è‚¡"
            except (ValueError, TypeError):
                sse_volume_str = "N/A"

            sse_results.append(f"  ğŸ“Š æˆäº¤é‡: {sse_volume_str}")

            # PE å’Œæ¢æ‰‹ç‡æ ¼å¼åŒ–
            try:
                avg_pe = float(sse_daily_dict["è‚¡ç¥¨"]["å¹³å‡å¸‚ç›ˆç‡"])
                avg_pe_str = f"{avg_pe:.2f}"
            except (ValueError, TypeError, KeyError):
                avg_pe_str = "N/A"
            sse_results.append(f"  ğŸ“ˆ å¹³å‡å¸‚ç›ˆç‡: {avg_pe_str}")

            try:
                turnover_rate = float(sse_daily_dict["è‚¡ç¥¨"]["æ¢æ‰‹ç‡"])
                turnover_rate_str = f"{turnover_rate:.2f}%"
            except (ValueError, TypeError, KeyError):
                turnover_rate_str = "N/A"
            sse_results.append(f"  ğŸ”„ æ¢æ‰‹ç‡: {turnover_rate_str}")

            results.append("\n".join(sse_results))
        except (KeyError, TypeError, ValueError) as e:
            logger.error(f"æ ¼å¼åŒ–ä¸Šäº¤æ‰€æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
            results.append(
                "ğŸ¦ ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€æ¦‚è§ˆ\n----------------------------\n  (éƒ¨åˆ†æ•°æ®æå–å¤±è´¥)"
            )
            errors.append("æ ¼å¼åŒ–ä¸Šäº¤æ‰€æ•°æ®å¤±è´¥")
    else:
        results.append(
            "ğŸ¦ ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€æ¦‚è§ˆ\n----------------------------\n  (æ•°æ®è·å–å¤±è´¥)"
        )

    results.append("")  # ç©ºè¡Œåˆ†éš”

    # --- æ ¼å¼åŒ–æ·±äº¤æ‰€æ•°æ® --- (åŸå§‹å•ä½ï¼šå…ƒ)
    if szse_summary_df is not None:
        szse_results = [f"ğŸ¦ æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€æ¦‚è§ˆ ({today_str})"]
        szse_results.append("----------------------------")
        try:
            szse_summary_df = szse_summary_df.set_index("è¯åˆ¸ç±»åˆ«")
            if "è‚¡ç¥¨" in szse_summary_df.index:
                stock_row = szse_summary_df.loc["è‚¡ç¥¨"]

                # ä½¿ç”¨ format_to_trillion å¹¶æŒ‡å®šåŸå§‹å•ä½ä¸º yuan
                szse_turnover_str = format_to_trillion(
                    stock_row["æˆäº¤é‡‘é¢"], original_unit="yuan"
                )
                szse_total_cap_str = format_to_trillion(
                    stock_row["æ€»å¸‚å€¼"], original_unit="yuan"
                )
                szse_flow_cap_str = format_to_trillion(
                    stock_row["æµé€šå¸‚å€¼"], original_unit="yuan"
                )

                szse_results.append(f"  ğŸ¢ è‚¡ç¥¨æ•°é‡: {int(stock_row['æ•°é‡']):,} å®¶")
                szse_results.append(f"  ğŸ’¸ è‚¡ç¥¨æˆäº¤é‡‘é¢: {szse_turnover_str}")
                szse_results.append(f"  ğŸ’° è‚¡ç¥¨æ€»å¸‚å€¼: {szse_total_cap_str}")
                szse_results.append(f"  ğŸ¦ æµé€šå¸‚å€¼: {szse_flow_cap_str}")
            else:
                szse_results.append("  (æœªæ‰¾åˆ°'è‚¡ç¥¨'ç±»åˆ«æ•°æ®)")

            results.append("\n".join(szse_results))
        except (KeyError, TypeError, ValueError) as e:
            logger.error(f"æ ¼å¼åŒ–æ·±äº¤æ‰€æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
            results.append(
                "ğŸ¦ æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€æ¦‚è§ˆ\n----------------------------\n  (éƒ¨åˆ†æ•°æ®æå–å¤±è´¥)"
            )
            errors.append("æ ¼å¼åŒ–æ·±äº¤æ‰€æ•°æ®å¤±è´¥")
    else:
        results.append(
            "ğŸ¦ æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€æ¦‚è§ˆ\n----------------------------\n  (æ•°æ®è·å–å¤±è´¥)"
        )

    # å¦‚æœæœ‰é”™è¯¯ï¼Œé™„åŠ é”™è¯¯ä¿¡æ¯
    if errors:
        results.append("\n" + "âš ï¸ æ³¨æ„: " + ", ".join(errors))

    return [{"text": "\n".join(results)}]


def format_large_number(num):
    if num is None or pd.isna(num):
        return "N/A"
    try:
        num = float(num)  # ç¡®ä¿æ˜¯æ•°å­—
    except (ValueError, TypeError):
        return "N/A"

    if abs(num) >= 1e12:  # ä¸‡äº¿
        return f"{num / 1e12:.2f} ä¸‡äº¿"
    elif abs(num) >= 1e8:  # äº¿
        return f"{num / 1e8:.2f} äº¿"
    elif abs(num) >= 1e4:  # ä¸‡
        return f"{num / 1e4:.2f} ä¸‡"
    else:
        return f"{num:,.2f}"  # å¸¦åƒä½åˆ†éš”ç¬¦ï¼Œä¿ç•™ä¸¤ä½å°æ•°


def safe_get_value(df, item_name, default="N/A"):
    try:
        value = df.loc[df["item"] == item_name, "value"].iloc[0]
        return value if pd.notna(value) else default
    except (IndexError, KeyError):
        return default


async def get_stock_details(cmd: str) -> List[Dict[str, str]]:
    stock_code = cmd.strip().split()[0] if cmd.strip() else None
    if not stock_code or not stock_code.isdigit():
        return [{"text": "âŒ é”™è¯¯ï¼šéœ€è¦æä¾›æœ‰æ•ˆçš„è‚¡ç¥¨ä»£ç ï¼ˆçº¯æ•°å­—ï¼‰ã€‚"}]

    # Determine market prefix for xueqiu API
    if stock_code.startswith("6"):
        xq_symbol = f"SH{stock_code}"
    elif stock_code.startswith(("0", "3")):
        xq_symbol = f"SZ{stock_code}"
    elif stock_code.startswith(("4", "8")):
        xq_symbol = f"BJ{stock_code}"  # Assuming BJ for Beijing Stock Exchange
    else:
        return [{"text": f"âŒ é”™è¯¯ï¼šæ— æ³•è¯†åˆ«çš„è‚¡ç¥¨ä»£ç æ ¼å¼ {stock_code}ã€‚"}]

    results = []
    stock_name = "N/A"  # Default name

    # --- Section 1: Basic Info (EM) ---
    try:
        logging.info(f"Fetching EM basic info for {stock_code}")
        stock_individual_info_em_df = await asyncio.to_thread(
            ak.stock_individual_info_em, symbol=stock_code
        )
        info_em_dict = stock_individual_info_em_df.set_index("item")["value"].to_dict()
        stock_name = info_em_dict.get(
            "è‚¡ç¥¨ç®€ç§°", stock_code
        )  # Use code if name not found
        industry = info_em_dict.get("è¡Œä¸š", "N/A")
        list_date_str = str(info_em_dict.get("ä¸Šå¸‚æ—¶é—´", "N/A"))
        list_date = (
            f"{list_date_str[:4]}-{list_date_str[4:6]}-{list_date_str[6:]}"
            if len(list_date_str) == 8
               and list_date_str.isdigit()  # Check if it's a valid date string
            else list_date_str
        )
        total_market_cap = format_large_number(info_em_dict.get("æ€»å¸‚å€¼", "N/A"))
        flow_market_cap = format_large_number(info_em_dict.get("æµé€šå¸‚å€¼", "N/A"))

        results.append(f"ğŸ¢ {stock_name} ({stock_code})")
        results.append("--------------------")
        results.append(f"   è¡Œä¸š: {industry}")
        results.append(f"   ä¸Šå¸‚: {list_date}")
        results.append(f"   æ€»å¸‚å€¼: {total_market_cap}")
        results.append(f"   æµé€šå€¼: {flow_market_cap}")

    except Exception as e:
        logging.error(f"Error fetching EM basic info for {stock_code}: {e}")
        # If basic info fails, still add header if possible, then the error
        if not results:  # Only add header if it wasn't added
            results.append(f"ğŸ¢ {stock_name} ({stock_code})")
            results.append("--------------------")
        results.append(f"âš ï¸ è·å–ä¸œæ–¹è´¢å¯ŒåŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")

    results.append("")  # Add a blank line separator

    # --- Section 2: Company Overview (XQ) ---
    try:
        logging.info(f"Fetching XQ company info for {xq_symbol}")
        stock_individual_basic_info_xq_df = await asyncio.to_thread(
            ak.stock_individual_basic_info_xq, symbol=xq_symbol
        )
        info_xq_dict = stock_individual_basic_info_xq_df.set_index("item")[
            "value"
        ].to_dict()
        main_business = info_xq_dict.get("main_operation_business", "N/A")
        controller = info_xq_dict.get("actual_controller", "N/A")

        results.append("â„¹ï¸ å…¬å¸æ¦‚å†µ")
        results.append("--------------------")
        # Keep business description concise
        business_display = (
            f"{main_business[:70]}..." if len(main_business) > 70 else main_business
        )
        results.append(f"   ä¸»è¥: {business_display}")
        results.append(f"   å®æ§: {controller}")

    except Exception as e:
        logging.error(f"Error fetching XQ company info for {xq_symbol}: {e}")
        results.append("â„¹ï¸ å…¬å¸æ¦‚å†µ")
        results.append("--------------------")
        results.append(f"âš ï¸ è·å–é›ªçƒå…¬å¸ä¿¡æ¯å¤±è´¥: {e}")

    results.append("")  # Add a blank line separator

    # --- Section 3: Realtime Quote & Bid/Ask (EM) ---
    try:
        logging.info(f"Fetching EM bid/ask info for {stock_code}")
        stock_bid_ask_em_df = await asyncio.to_thread(
            ak.stock_bid_ask_em, symbol=stock_code
        )
        bid_ask_dict = stock_bid_ask_em_df.set_index("item")["value"].to_dict()

        latest_price = bid_ask_dict.get("æœ€æ–°", "N/A")
        price_change = bid_ask_dict.get("æ¶¨è·Œ", "N/A")
        change_percent = bid_ask_dict.get("æ¶¨å¹…", "N/A")
        high_price = bid_ask_dict.get("æœ€é«˜", "N/A")
        low_price = bid_ask_dict.get("æœ€ä½", "N/A")
        open_price = bid_ask_dict.get("ä»Šå¼€", "N/A")
        prev_close = bid_ask_dict.get("æ˜¨æ”¶", "N/A")
        volume = format_large_number(
            bid_ask_dict.get("æ€»æ‰‹", 0)
            * 100  # EM æ€»æ‰‹ is lots, multiply by 100 for shares
        )
        turnover = format_large_number(bid_ask_dict.get("é‡‘é¢", "N/A"))
        turnover_rate = bid_ask_dict.get("æ¢æ‰‹", "N/A")
        volume_ratio = bid_ask_dict.get("é‡æ¯”", "N/A")

        # Format price related numbers
        latest_price_f = (
            f"{latest_price:.2f}"
            if isinstance(latest_price, (int, float))
            else latest_price
        )
        open_price_f = (
            f"{open_price:.2f}" if isinstance(open_price, (int, float)) else open_price
        )
        prev_close_f = (
            f"{prev_close:.2f}" if isinstance(prev_close, (int, float)) else prev_close
        )
        high_price_f = (
            f"{high_price:.2f}" if isinstance(high_price, (int, float)) else high_price
        )
        low_price_f = (
            f"{low_price:.2f}" if isinstance(low_price, (int, float)) else low_price
        )
        turnover_rate_f = (
            f"{turnover_rate:.2f}%"
            if isinstance(turnover_rate, (int, float))
            else turnover_rate
        )

        # Determine emoji and format change/percent
        price_emoji = "âšªï¸"
        price_change_f = "N/A"
        change_percent_f = "N/A"
        if isinstance(price_change, (int, float)) and isinstance(
                change_percent, (int, float)
        ):
            if price_change > 0:
                price_emoji = "ğŸ”¼"
                price_change_f = f"+{price_change:.2f}"
                change_percent_f = f"+{change_percent:.2f}%"
            elif price_change < 0:
                price_emoji = "ğŸ”½"
                price_change_f = f"{price_change:.2f}"
                change_percent_f = f"{change_percent:.2f}%"
            else:
                price_change_f = f"{price_change:.2f}"
                change_percent_f = f"{change_percent:.2f}%"

        results.append("ğŸ“ˆ å®æ—¶è¡Œæƒ…")
        results.append("--------------------")
        results.append(
            f"   {price_emoji} {latest_price_f} ({price_change_f} / {change_percent_f})"
        )
        results.append(f"   ä»Šå¼€: {open_price_f} | æ˜¨æ”¶: {prev_close_f}")
        results.append(f"   æœ€é«˜: {high_price_f} | æœ€ä½: {low_price_f}")
        results.append(
            f"   æˆäº¤é‡: {volume} è‚¡"
        )  # Changed 'æ‰‹' to 'è‚¡' after multiplying
        results.append(f"   æˆäº¤é¢: {turnover}")
        results.append(f"   æ¢æ‰‹ç‡: {turnover_rate_f} | é‡æ¯”: {volume_ratio}")

        results.append("")  # Blank line before bid/ask

        # --- Bid/Ask ---
        results.append("ğŸ“Š ä¹°å–ç›˜")
        # Sell side
        sell_lines = []
        for i in range(5, 0, -1):
            price = bid_ask_dict.get(f"sell_{i}", "-")
            vol = format_large_number(bid_ask_dict.get(f"sell_{i}_vol", 0))
            price_f = f"{price:.2f}" if isinstance(price, (int, float)) else price
            sell_lines.append(f"   å–{i}: {price_f} ({vol} è‚¡)")
        results.extend(sell_lines)

        results.append("   -----------")  # Separator

        # Buy side
        buy_lines = []
        for i in range(1, 6):
            price = bid_ask_dict.get(f"buy_{i}", "-")
            vol = format_large_number(bid_ask_dict.get(f"buy_{i}_vol", 0))
            price_f = f"{price:.2f}" if isinstance(price, (int, float)) else price
            buy_lines.append(f"   ä¹°{i}: {price_f} ({vol} è‚¡)")
        results.extend(buy_lines)

    except Exception as e:
        logging.error(f"Error fetching EM bid/ask info for {stock_code}: {e}")
        # Add section header even if fetch fails
        results.append("ğŸ“ˆ å®æ—¶è¡Œæƒ… & ğŸ“Š ä¹°å–ç›˜")
        results.append("--------------------")
        results.append(f"âš ï¸ è·å–ä¸œæ–¹è´¢å¯Œè¡Œæƒ…æŠ¥ä»·å¤±è´¥: {e}")

    # Final check if anything was added at all
    if len(results) <= 2:  # Only header and separator potentially
        return [{"text": f"âŒ æœªèƒ½è·å–è‚¡ç¥¨ {stock_code} çš„ä»»ä½•æœ‰æ•ˆä¿¡æ¯ã€‚"}]

    return [{"text": "\n".join(results)}]


async def get_financial_report(cmd: str) -> List[Dict[str, str]]:
    """è·å–ä»Šæ—¥æ‰€æœ‰è´¢æŠ¥å‘å¸ƒä¿¡æ¯"""
    try:
        today_date = datetime.date.today().strftime("%Y%m%d")
        logger.info(f"æ­£åœ¨æŸ¥è¯¢æ—¥æœŸ {today_date} çš„è´¢æŠ¥å‘å¸ƒä¿¡æ¯...")
        report_df = ak.news_report_time_baidu(date=today_date)

        if report_df.empty:
            return [{"text": f"â„¹ï¸ ä»Šæ—¥ï¼ˆ{today_date}ï¼‰æ— è´¢æŠ¥å‘å¸ƒä¿¡æ¯ã€‚"}]
        reports = [f"ğŸ“… ä»Šæ—¥ ({today_date}) è´¢æŠ¥å‘å¸ƒè®¡åˆ’:"]
        reports.append("-------------------------------------")
        # ä½¿ç”¨åŸå§‹çš„ report_df è¿­ä»£
        for index, row in report_df.iterrows():
            code = row.get("è‚¡ç¥¨ä»£ç ", "æœªçŸ¥ä»£ç ")
            name = row.get("è‚¡ç¥¨ç®€ç§°", "æœªçŸ¥ç®€ç§°")
            period = row.get("è´¢æŠ¥æœŸ", "æœªçŸ¥å‘¨æœŸ")
            reports.append(f"â–ªï¸ {name} ({code}) - {period}")

        response_text = "\n".join(reports)

        # æ£€æŸ¥æ¶ˆæ¯é•¿åº¦
        if len(response_text) > 1800:  # ç•™ä¸€äº›ä½™é‡
            logger.warning(f"è´¢æŠ¥ä¿¡æ¯è¿‡é•¿ ({len(response_text)} chars)ï¼Œè¿›è¡Œæˆªæ–­ã€‚")
            # ä¿ç•™æ ‡é¢˜å’Œéƒ¨åˆ†å†…å®¹
            lines_to_keep = 2 + int(1700 / 20)  # ä¼°ç®—æ¯è¡Œ20å­—ç¬¦
            response_text = (
                    "\n".join(reports[:lines_to_keep]) + "\n... (å†…å®¹è¿‡é•¿å·²æˆªæ–­)"
            )

        return [{"text": response_text}]
    except Exception as e:
        logger.error(f"è·å–è´¢æŠ¥ä¿¡æ¯æ—¶å‡ºé”™: {e}", exc_info=True)
        return [{"text": f"âŒ è·å–è´¢æŠ¥ä¿¡æ¯æ—¶å‡ºé”™: {e}"}]


class StockPlugin(BasePlugin):
    name = "StockPlugin"  # æ’ä»¶åç§°
    version = "0.0.1"  # æ’ä»¶ç‰ˆæœ¬

    config = None
    config_path = None
    config_last_modified = 0

    async def on_load(self):
        """æ’ä»¶åŠ è½½æ—¶æ‰§è¡Œçš„æ“ä½œ"""
        logger.info(f"{self.name} æ’ä»¶å·²åŠ è½½")
        logger.info(f"æ’ä»¶ç‰ˆæœ¬: {self.version}")

        # åˆå§‹åŒ–é…ç½®è·¯å¾„
        self.config_path = Path(__file__).parent / "config" / "config.toml"

        # åŠ è½½é…ç½®
        self.load_config()

    def load_config(self) -> None:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if self.config_path.exists():
                with open(self.config_path, "rb") as f:
                    config_data = tomllib.load(f)
                    self.config = Config.from_dict(config_data)
                self.config_last_modified = os.path.getmtime(self.config_path)
                logger.info(f"æˆåŠŸåŠ è½½ {self.name} é…ç½®")
            else:
                logger.warning(f"{self.name} é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
                self.config = Config([], [])  # Corrected initialization
        except Exception as e:
            logger.error(f"åŠ è½½ {self.name} é…ç½®å‡ºé”™: {str(e)}")
            self.config = Config([], [])  # Corrected initialization

    def check_config_update(self) -> bool:
        """æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å·²æ›´æ–°"""
        try:
            if self.config_path.exists():
                last_modified = os.path.getmtime(self.config_path)
                if last_modified > self.config_last_modified:
                    logger.info(f"{self.name} é…ç½®æ–‡ä»¶å·²æ›´æ–°ï¼Œé‡æ–°åŠ è½½")
                    self.load_config()
                    return True
            return False
        except Exception as e:
            logger.error(f"æ£€æŸ¥ {self.name} é…ç½®æ›´æ–°å‡ºé”™: {str(e)}")
            return False

    def is_user_authorized(self, user_id: int, group_id: Optional[int] = None) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰æƒé™ä½¿ç”¨æ­¤æ’ä»¶"""
        if not self.config:
            logger.warning("æˆæƒæ£€æŸ¥å¤±è´¥ï¼šé…ç½®æœªåŠ è½½ã€‚")
            return False

        # æ£€æŸ¥ç”¨æˆ·IDæ˜¯å¦åœ¨ç™½åå•ä¸­
        if user_id in self.config.whitelist_users:
            return True

        # å¦‚æœæä¾›äº†ç¾¤ç»„IDï¼Œæ£€æŸ¥ç¾¤ç»„æ˜¯å¦åœ¨ç™½åå•ä¸­
        if group_id and group_id in self.config.whitelist_groups:
            return True

        logger.debug(f"ç”¨æˆ· {user_id} (ç¾¤: {group_id}) æœªæˆæƒã€‚")
        return False

    @bot.group_event()
    async def on_group_event(self, msg: GroupMessage):
        """å¤„ç†ç¾¤æ¶ˆæ¯äº‹ä»¶"""
        self.check_config_update()  # Check for config updates on each event

        if not msg.raw_message.startswith("è‚¡ç¥¨ "):
            return

        if not self.is_user_authorized(msg.user_id, msg.group_id):
            await self.api.post_group_msg(
                msg.group_id, text="ğŸš« æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨è‚¡ç¥¨æ’ä»¶"
            )
            return

        command_full = msg.raw_message[3:].strip()
        if not command_full:
            await self.api.post_group_msg(
                msg.group_id,
                text="â„¹ï¸ è¯·è¾“å…¥æ“ä½œæŒ‡ä»¤ï¼Œä¾‹å¦‚ï¼šè‚¡ç¥¨ å†å² 600519",
            )
            return

        command_handlers = {
            "å†å²": handle_historical_command,
            "å®æ—¶": get_stock_realtime_data,
            "æ–°é—»": get_stock_news,
            "é¢„æµ‹": get_stock_deepseek_prediction,
            "æ€»è²Œ": get_market_overview,
            "ä¸ªè‚¡": get_stock_details,
            "è´¢æŠ¥": get_financial_report,
        }

        parts = command_full.split()
        command_keyword = parts[0] if parts else ""
        cmd_args = " ".join(parts[1:])

        if command_keyword in command_handlers:
            handler = command_handlers[command_keyword]
            try:
                # è°ƒç”¨å¤„ç†å™¨è·å–å¾…å‘é€æ¶ˆæ¯åˆ—è¡¨
                messages_to_send = await handler(cmd_args)

                # è¿­ä»£å¤„ç†æ¶ˆæ¯åˆ—è¡¨
                for msg_data in messages_to_send:
                    try:
                        if "text" in msg_data:
                            await self.api.post_group_msg(
                                msg.group_id, text=msg_data["text"]
                            )
                        elif "image" in msg_data:
                            filepath = msg_data["image"]
                            await self.api.post_group_msg(msg.group_id, image=filepath)
                            logger.info(f"æˆåŠŸå‘é€å›¾ç‰‡: {filepath}")
                    except Exception as send_error:
                        logger.error(f"å‘é€æ¶ˆæ¯å¤±è´¥: {send_error}", exc_info=True)
                        # å°è¯•å‘é€ä¸€æ¡é”™è¯¯æç¤ºç»™ç”¨æˆ·
                        try:
                            await self.api.post_group_msg(
                                msg.group_id, text=f"âŒ å‘é€éƒ¨åˆ†ç»“æœæ—¶å‡ºé”™ã€‚"
                            )
                        except Exception:
                            logger.error("å‘é€é”™è¯¯æç¤ºä¹Ÿå¤±è´¥äº†ã€‚")
            except Exception as handler_error:
                logger.error(
                    f"æ‰§è¡Œå‘½ä»¤ '{command_keyword}' å¤„ç†å™¨æ—¶å‡ºé”™: {handler_error}",
                    exc_info=True,
                )
                await self.api.post_group_msg(
                    msg.group_id,
                    text=f"âŒ æ‰§è¡Œå‘½ä»¤ '{command_keyword}' æ—¶é‡åˆ°å†…éƒ¨é”™è¯¯ã€‚",
                )
        else:
            # å¦‚æœå‘½ä»¤æœªè¢«è¯†åˆ«
            supported_commands = ", ".join(command_handlers.keys())
            await self.api.post_group_msg(
                msg.group_id,
                text=f"â“ æ— æ³•è¯†åˆ«å‘½ä»¤ '{command_keyword}'ã€‚\næ”¯æŒï¼š{supported_commands}ã€‚\n"
                     f"ç¤ºä¾‹ï¼šè‚¡ç¥¨ å†å² 600519 | è‚¡ç¥¨ å®æ—¶ 000001",
            )
